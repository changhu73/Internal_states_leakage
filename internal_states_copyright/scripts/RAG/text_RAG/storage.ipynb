{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guangwei/miniconda3/envs/zdh/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Encoding inputs: 100%|██████████| 238/238 [00:09<00:00, 25.84it/s]\n",
      "WARNING clustering 1898 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 1898 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the quantized index on input vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 1898 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 1898 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 1898 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 1898 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 1898 points to 256 centroids: please provide at least 9984 training points\n",
      "WARNING clustering 1898 points to 256 centroids: please provide at least 9984 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index training completed.\n",
      "Adding input vectors to the quantized index...\n",
      "Input vectors added to the quantized index.\n",
      "Quantized FAISS index has been saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Set device for CUDA\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
    "model = model.to(device)\n",
    "\n",
    "# Load JSON data and prepare pairs\n",
    "def load_data(non_infringement_file, infringement_file):\n",
    "    with open(non_infringement_file, 'r', encoding='utf-8') as file:\n",
    "        non_infringement_json_data = json.load(file)\n",
    "\n",
    "    # Extract input and reference text for non-infringement\n",
    "    non_infringement_inputs = [entry['input'] for entry in non_infringement_json_data]\n",
    "    non_infringement_references = [entry['reference'] for entry in non_infringement_json_data]\n",
    "\n",
    "    with open(infringement_file, 'r', encoding='utf-8') as file:\n",
    "        infringement_json_data = json.load(file)\n",
    "\n",
    "    # Extract input and reference text for infringement\n",
    "    infringement_inputs = [entry['input'] for entry in infringement_json_data]\n",
    "    infringement_references = [entry['reference'] for entry in infringement_json_data]\n",
    "\n",
    "    # Create structured matching pairs\n",
    "    non_infringement_pairs = list(zip(non_infringement_inputs, non_infringement_references))\n",
    "    infringement_pairs = list(zip(infringement_inputs, infringement_references))\n",
    "\n",
    "    # Combine all pairs into a single list\n",
    "    all_pairs = non_infringement_pairs + infringement_pairs\n",
    "    return all_pairs\n",
    "\n",
    "# Example usage\n",
    "all_pairs = load_data(\n",
    "    ' test_division/extra_30.non_infringement.json',\n",
    "    ' test_division/extra_30.infringement.json'\n",
    ")\n",
    "\n",
    "# Extract `input` texts and `references` for storage\n",
    "input_texts = [pair[0] for pair in all_pairs]\n",
    "references = [pair[1] for pair in all_pairs]\n",
    "\n",
    "# Encode `input` texts in batches\n",
    "def batch_encode_texts(model, texts, batch_size=8):\n",
    "    all_vectors = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding inputs\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_vectors = model.encode(batch, convert_to_tensor=True, device=device)\n",
    "        all_vectors.append(batch_vectors.cpu().numpy())\n",
    "    return np.vstack(all_vectors)\n",
    "\n",
    "# Encode the `input` texts\n",
    "input_vectors = batch_encode_texts(model, input_texts)\n",
    "\n",
    "# Initialize FAISS index with Product Quantization (PQ)\n",
    "dimension = input_vectors.shape[1]\n",
    "nlist = 3  # Number of clusters (for coarse quantization)\n",
    "m = 8  # Number of subquantizers\n",
    "n_bits = 8  # Bits per subquantizer\n",
    "\n",
    "quantizer = faiss.IndexFlatL2(dimension)  # Use flat index for quantization\n",
    "gpu_index = faiss.IndexIVFPQ(quantizer, dimension, nlist, m, n_bits)\n",
    "\n",
    "# Train the index on input vectors\n",
    "print(\"Training the quantized index on input vectors...\")\n",
    "gpu_index.train(input_vectors)\n",
    "print(\"Index training completed.\")\n",
    "\n",
    "# Add input vectors to the quantized index\n",
    "print(\"Adding input vectors to the quantized index...\")\n",
    "gpu_index.add(input_vectors)\n",
    "print(\"Input vectors added to the quantized index.\")\n",
    "\n",
    "# Save the quantized FAISS index\n",
    "faiss.write_index(gpu_index, 'faiss_index_quantized.index')\n",
    "print(\"Quantized FAISS index has been saved.\")\n",
    "\n",
    "# Setting up SQLite database to store documents and embeddings\n",
    "def setup_database():\n",
    "    conn = sqlite3.connect('rag_db.sqlite')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(''' \n",
    "        CREATE TABLE IF NOT EXISTS documents (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            document_text TEXT NOT NULL,\n",
    "            reference_text TEXT NOT NULL  -- Add reference_text column to store references\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "# Store documents and references in the database\n",
    "def store_documents(conn, inputs, references):\n",
    "    cursor = conn.cursor()\n",
    "    for inp, ref in zip(inputs, references):\n",
    "        cursor.execute('INSERT INTO documents (document_text, reference_text) VALUES (?, ?)', \n",
    "                       (inp, ref))  # Store input and reference\n",
    "    conn.commit()\n",
    "\n",
    "# Store documents and references in the database\n",
    "conn = setup_database()\n",
    "store_documents(conn, input_texts, references)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zdh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
